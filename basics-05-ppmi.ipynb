{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pointwise Mutual Information (PMI) and Positive PMI (PPMI)\n",
        "In statistics, probability theory, and information theory, **Pointwise Mutual Information (PMI)** is a measure of association between two events. It compares the probability of two events occurring together to the probability we would expect if the events were independent.\n",
        "\n",
        "The formula for PMI is:\n",
        "$$ \\text{PMI}(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x) \\cdot P(y)} \\right), $$\n",
        "\n",
        "where:\n",
        "- $P(w)$ is the probability of seeing word $w$ in the corpus.\n",
        "- $P(w_1, w_2)$ is the probability that $w_1$ and $w_2$ occur next to each other (i.e., co-occur as a word pair).\n",
        "\n",
        "\n",
        "If two words occur together more often than expected by chance, the PMI is positive. If they co-occur less frequently, it is negative.\n",
        "\n",
        "In Natural Language Processing, PMI is used to identify meaningful word associations. For example, \"natural\" and \"language\" often appear together, so their PMI would be high.\n",
        "\n",
        "To focus on strong, positive associations, we use **Positive PMI (PPMI)**, which sets all negative PMI values to zero:\n",
        "\n",
        "$$ \\text{PPMI}(x, y) = \\max(\\text{PMI}(x, y),\\ 0) $$\n",
        "\n",
        "\n",
        "This variant is widely used in NLP tasks like word embedding and semantic similarity analysis.\n",
        "\n",
        "### This Notebook\n",
        "In this notebook, we perform a basic co-occurrence analysis on a small corpus. After tokenizing and removing stopwords, we count word frequencies and adjacent co-occurrences to compute word and pair probabilities. Using these, we calculate PMI and PPMI scores and rank word pairs by their associative strength."
      ],
      "metadata": {
        "id": "iWjXaVP-MvXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminaries\n",
        "\n",
        "This installs required python packages and nltk resources."
      ],
      "metadata": {
        "id": "Cn_W7w6yNZiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True);"
      ],
      "metadata": {
        "id": "CMNAWG7OxSY-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Corpus"
      ],
      "metadata": {
        "id": "GU0dITSCNdR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def get_corpus_stats(corpus: list[str]) -> tuple[int, int, dict[str, int], dict[tuple[str, str], int]]:\n",
        "    word_count = defaultdict(lambda: 0)\n",
        "    cooccurrence_count = defaultdict(lambda: 0)\n",
        "\n",
        "    for document in corpus:\n",
        "        words = [w for w in word_tokenize(document) if w not in stopwords]\n",
        "        for i, word in enumerate(words):\n",
        "            word_count[word] += 1\n",
        "            if i < len(words) - 1:\n",
        "                pair = (word, words[i + 1])\n",
        "                cooccurrence_count[pair] += 1\n",
        "    total_tokens = sum(word_count.values())\n",
        "    total_cooccurrences = sum(cooccurrence_count.values())\n",
        "\n",
        "    return {w: c/total_tokens for w, c in word_count.items()}, {ws: c/total_cooccurrences for ws, c in cooccurrence_count.items()}\n",
        "\n",
        "wordprob, coocprob = get_corpus_stats([\n",
        "  \"I love natural language processing.\",\n",
        "  \"Deep learning is revolutionizing natural language processing.\",\n",
        "  \"Natural language processing is a core part of artificial intelligence.\",\n",
        "  \"Machine learning and natural language processing go hand in hand.\",\n",
        "  \"I love deep learning and artificial intelligence.\",\n",
        "])\n"
      ],
      "metadata": {
        "id": "u8DR_iOzvaQD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing PMI and PPMI"
      ],
      "metadata": {
        "id": "Iu2efn41NhoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vu8iMkOliJSv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def pmi(x, y):\n",
        "    p_xy = coocprob.get((x, y), coocprob.get((y, x), 0))\n",
        "    if p_xy == 0:\n",
        "        return float('-inf')  # PMI is undefined (or -inf) for zero co-occurrence\n",
        "    return math.log2(p_xy / (wordprob[x] * wordprob[y]))\n",
        "\n",
        "def ppmi(x, y):\n",
        "  return max((0, pmi(x, y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstration"
      ],
      "metadata": {
        "id": "jPWYq3hNNmxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def rank_by_pmi(words: list[str]):\n",
        "  return sorted(list((w1, w2, ppmi(w1, w2))for w1, w2 in itertools.combinations(words, 2)), key=lambda x: x[2], reverse=True)\n",
        "\n",
        "rank_by_pmi([\"natural\", \"language\", \"processing\", \"deep\", \"learning\"])[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMOXdr0ditH3",
        "outputId": "c8e8cad7-d4fa-4393-d395-006153caa05b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('deep', 'learning', 3.8339442305367433),\n",
              " ('natural', 'language', 3.4189067312578993),\n",
              " ('language', 'processing', 3.4189067312578993),\n",
              " ('natural', 'learning', 2.2489817298155867),\n",
              " ('natural', 'processing', 0),\n",
              " ('natural', 'deep', 0),\n",
              " ('language', 'deep', 0),\n",
              " ('language', 'learning', 0),\n",
              " ('processing', 'deep', 0),\n",
              " ('processing', 'learning', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}